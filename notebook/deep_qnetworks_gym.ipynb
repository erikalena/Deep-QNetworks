{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep QNetworks\n",
    "\n",
    "Deep Qlearning with experience replay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "things to do:\n",
    "- correct render methods in env\n",
    "- provide info about body in buffer and as output of step\n",
    "- create separate function for render and epsilon_greedy (use get_obs for all the info you need)\n",
    "- merge these functions and some rules of the game in a separate class Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/alexserra98/uni/r_l/project/Deep-QNetworks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from src_code.cnn.buffers import SeqReplayBuffer\n",
    "from src_code.cnn.agent import DQN, SnakeAgent\n",
    "from src_code.cnn.environment import SnakeEnv\n",
    "from src_code.cnn.train import train_step\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import pickle\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from PIL import Image\n",
    "from typing import Dict, List, Tuple, Union\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiliaze Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    current_time: str = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    description: str = (\n",
    "        \"Deep Q-Network Snake with gym and class agent, done after snake eats itself\"\n",
    "    )\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    batch_size: int = 32  # Size of batch taken from replay buffer\n",
    "    env_size_x: int = 10\n",
    "    env_size_y: int = 10\n",
    "    num_envs: int = 1\n",
    "    max_steps_per_episode: int = 30000\n",
    "    max_num_episodes: int = 20000\n",
    "    deque_size: int = 100\n",
    "\n",
    "    # epsilon\n",
    "    epsilon_max: float = 1.0  # Maximum epsilon greedy parameter\n",
    "    epsilon_min: float = 0.2  # Minimum epsilon greedy parameter\n",
    "\n",
    "    no_back = False\n",
    "    done_on_collision: bool = False\n",
    "    update_after_actions: int = 4  # Train the model after 4 actions\n",
    "    update_target_network: int = 10000  # How often to update the target network\n",
    "    epsilon_random_frames: int = 100000  # Number of frames for exploration\n",
    "    eps_decay_frames: int = 10000\n",
    "    buffer_size: int = 100000  # Size of the replay buffer\n",
    "    eps_decay_rate: float = 0.001\n",
    "    reward: Dict[str, int] =  {\"eat\": 40, \"dead\": -1, \"step\": 0}\n",
    "    \n",
    "\n",
    "    output_filename: str = \"log.json\"\n",
    "    output_logdir: str = \"results/orfeo\"\n",
    "    output_checkpoint_dir: str = \"checkpoint/orfeo\"\n",
    "    save_step: int = 100  # Save model every 100 episodes and log results\n",
    "    logging_level: int = logging.DEBUG\n",
    "    load_checkpoint: str =  \"checkpoint/18839/model_8299\"\n",
    "\n",
    "\n",
    "CONFIG = Config()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Optimizer not found, loading default optimizer\n",
      "WARNING:root:Checkpoint not found, loading default model\n",
      "  0%|          | 0/20000 [13:18<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 77\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m     65\u001b[0m     \u001b[39mlen\u001b[39m(buffer) \u001b[39m>\u001b[39m CONFIG\u001b[39m.\u001b[39mbatch_size\n\u001b[1;32m     66\u001b[0m     \u001b[39mand\u001b[39;00m cur_frame \u001b[39m%\u001b[39m CONFIG\u001b[39m.\u001b[39mupdate_after_actions \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     67\u001b[0m ):\n\u001b[1;32m     68\u001b[0m     (\n\u001b[1;32m     69\u001b[0m         states,\n\u001b[1;32m     70\u001b[0m         actions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m         new_bodies,\n\u001b[1;32m     76\u001b[0m     ) \u001b[39m=\u001b[39m buffer\u001b[39m.\u001b[39msample(CONFIG\u001b[39m.\u001b[39mbatch_size)\n\u001b[0;32m---> 77\u001b[0m     loss \u001b[39m=\u001b[39m train_step(\n\u001b[1;32m     78\u001b[0m         states,\n\u001b[1;32m     79\u001b[0m         actions,\n\u001b[1;32m     80\u001b[0m         rewards,\n\u001b[1;32m     81\u001b[0m         next_states,\n\u001b[1;32m     82\u001b[0m         dones,\n\u001b[1;32m     83\u001b[0m         bodies,\n\u001b[1;32m     84\u001b[0m         new_bodies,\n\u001b[1;32m     85\u001b[0m         snake_agent,\n\u001b[1;32m     86\u001b[0m         loss_function,\n\u001b[1;32m     87\u001b[0m         optimizer,\n\u001b[1;32m     88\u001b[0m         CONFIG\u001b[39m.\u001b[39;49mdevice,\n\u001b[1;32m     89\u001b[0m     )\n\u001b[1;32m     91\u001b[0m \u001b[39m# Update target network every update_target_network steps.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m# if cur_frame % CONFIG.update_target_network == 0:\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39m#     snake_agent.model_target.load_state_dict(snake_agent.model.state_dict())\u001b[39;00m\n\u001b[1;32m     95\u001b[0m timestep \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/uni/r_l/project/Deep-QNetworks/src_code/cnn/train.py:24\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(states, actions, rewards, next_states, dones, bodies, new_bodies, snake_agent, loss_function, optimizer, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mPerform a training iteration on a batch of data sampled from the experience\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mreplay buffer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m    - discount: the discount factor, standard discount factor in RL to evaluate less long term rewards\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m# compute targets for Q-learning\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m# the max Q-value of the next state is the target for the current state\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m# the image to be fed to the network is a grey scale image of the world\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m images \u001b[39m=\u001b[39m [snake_agent\u001b[39m.\u001b[39mget_image(next_state, new_body) \u001b[39mfor\u001b[39;00m next_state, new_body \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(next_states,new_bodies)]\n\u001b[1;32m     25\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(np\u001b[39m.\u001b[39marray(images), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     26\u001b[0m max_next_qs \u001b[39m=\u001b[39m snake_agent\u001b[39m.\u001b[39mmodel_target(\u001b[39minput\u001b[39m)\u001b[39m.\u001b[39mmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mvalues\n",
      "File \u001b[0;32m~/uni/r_l/project/Deep-QNetworks/src_code/cnn/train.py:24\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mPerform a training iteration on a batch of data sampled from the experience\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mreplay buffer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m    - discount: the discount factor, standard discount factor in RL to evaluate less long term rewards\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m# compute targets for Q-learning\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m# the max Q-value of the next state is the target for the current state\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m# the image to be fed to the network is a grey scale image of the world\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m images \u001b[39m=\u001b[39m [snake_agent\u001b[39m.\u001b[39;49mget_image(next_state, new_body) \u001b[39mfor\u001b[39;00m next_state, new_body \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(next_states,new_bodies)]\n\u001b[1;32m     25\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(np\u001b[39m.\u001b[39marray(images), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     26\u001b[0m max_next_qs \u001b[39m=\u001b[39m snake_agent\u001b[39m.\u001b[39mmodel_target(\u001b[39minput\u001b[39m)\u001b[39m.\u001b[39mmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mvalues\n",
      "File \u001b[0;32m~/uni/r_l/project/Deep-QNetworks/src_code/cnn/agent.py:115\u001b[0m, in \u001b[0;36mSnakeAgent.get_image\u001b[0;34m(self, state, body)\u001b[0m\n\u001b[1;32m    112\u001b[0m     image[\u001b[39mint\u001b[39m(state[\u001b[39m2\u001b[39m]), \u001b[39mint\u001b[39m(state[\u001b[39m3\u001b[39m])] \u001b[39m=\u001b[39m \u001b[39m.5\u001b[39m\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m state[\u001b[39m0\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m state[\u001b[39m0\u001b[39m] \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize[\u001b[39m0\u001b[39m] \u001b[39mand\u001b[39;00m state[\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m state[\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize[\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> 115\u001b[0m     image[\u001b[39mint\u001b[39;49m(state[\u001b[39m0\u001b[39;49m]), \u001b[39mint\u001b[39m(state[\u001b[39m1\u001b[39m])] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    116\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[39m# if the agent is out of the world, it is dead and so we cancel the food as well\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39m# this check is just for safety reasons, if we allow the snake to go through the walls\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[39m# this should never happen\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     image[\u001b[39mint\u001b[39m(state[\u001b[39m2\u001b[39m]), \u001b[39mint\u001b[39m(state[\u001b[39m3\u001b[39m])] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = SnakeEnv(size=(CONFIG.env_size_x, CONFIG.env_size_y), config=CONFIG)\n",
    "\n",
    "epsilon_decay = (CONFIG.epsilon_max / (CONFIG.max_num_episodes * 0.5)) * 100\n",
    "\n",
    "snake_agent = SnakeAgent(\n",
    "    initial_epsilon=CONFIG.epsilon_max,\n",
    "    final_epsilon=CONFIG.epsilon_min,\n",
    "    epsilon_decay=CONFIG.eps_decay_rate,\n",
    "    num_actions=env.action_space.n,  # type: ignore\n",
    "    env=env,\n",
    "    size=(CONFIG.env_size_x, CONFIG.env_size_y),\n",
    "    device=CONFIG.device,\n",
    ")\n",
    "\n",
    "buffer = SeqReplayBuffer(size=CONFIG.buffer_size, device=CONFIG.device)\n",
    "\n",
    "optimizer = torch.optim.Adam(snake_agent.model.parameters(), lr=0.00025)\n",
    "if CONFIG.load_checkpoint is not None:\n",
    "    optimizer = snake_agent.load_model(CONFIG.load_checkpoint, optimizer=optimizer)\n",
    "# huber loss\n",
    "loss_function = nn.HuberLoss()\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=CONFIG.deque_size)  # type: ignore\n",
    "\n",
    "cur_frame = 0\n",
    "fruits_eaten_per_episode = []\n",
    "pbar = tqdm(total=CONFIG.max_num_episodes)\n",
    "tot_step = 0\n",
    "for episode in range(CONFIG.max_num_episodes):\n",
    "    obs, info = copy.deepcopy(env.reset())\n",
    "    terminated = False\n",
    "    timestep = 0\n",
    "    frames = []\n",
    "    rewards_per_episode = []\n",
    "    n_step_per_episode = []\n",
    "    while not terminated:\n",
    "        cur_frame += 1\n",
    "        tot_step += 1\n",
    "        action = snake_agent.get_action(obs, info)\n",
    "        new_obs, reward, done, selected_action, new_info = env.step(action)\n",
    "        terminated = done or (timestep > CONFIG.max_steps_per_episode)\n",
    "\n",
    "        #\n",
    "        rewards_per_episode.append(reward)\n",
    "        tmp_state = np.concatenate((obs[\"agent\"], obs[\"target\"]))\n",
    "        tmp_body = info[\"body\"]\n",
    "\n",
    "        if timestep < 1002 or timestep > CONFIG.max_steps_per_episode - 502:\n",
    "            frame = snake_agent.get_image(tmp_state, tmp_body)\n",
    "            frames.append(frame)\n",
    "        # Save actions and states in replay buffer\n",
    "        buffer.add(obs, selected_action, reward, new_obs, done, info, new_info)\n",
    "\n",
    "        # Update obs and info\n",
    "        obs = copy.deepcopy(new_obs)\n",
    "        info = copy.deepcopy(new_info)\n",
    "\n",
    "        cur_frame += 1\n",
    "\n",
    "        if tot_step > CONFIG.epsilon_random_frames :\n",
    "            if tot_step % CONFIG.eps_decay_frames == 0:\n",
    "                snake_agent.decay_epsilon()\n",
    "\n",
    "        # Train neural network.\n",
    "        if (\n",
    "            len(buffer) > CONFIG.batch_size\n",
    "            and cur_frame % CONFIG.update_after_actions == 0\n",
    "        ):\n",
    "            (\n",
    "                states,\n",
    "                actions,\n",
    "                rewards,\n",
    "                next_states,\n",
    "                dones,\n",
    "                bodies,\n",
    "                new_bodies,\n",
    "            ) = buffer.sample(CONFIG.batch_size)\n",
    "            loss = train_step(\n",
    "                states,\n",
    "                actions,\n",
    "                rewards,\n",
    "                next_states,\n",
    "                dones,\n",
    "                bodies,\n",
    "                new_bodies,\n",
    "                snake_agent,\n",
    "                loss_function,\n",
    "                optimizer,\n",
    "                CONFIG.device,\n",
    "            )\n",
    "\n",
    "        # Update target network every update_target_network steps.\n",
    "        # if cur_frame % CONFIG.update_target_network == 0:\n",
    "        #     snake_agent.model_target.load_state_dict(snake_agent.model.state_dict())\n",
    "\n",
    "        timestep += 1\n",
    "        # if env.eaten_fruits == 10:\n",
    "        n_step_per_episode.append(timestep)\n",
    "        #     break\n",
    "    # if episode>CONFIG.epsilon_random_frames:\n",
    "    # snake_agent.decay_epsilon()\n",
    "\n",
    "    fruits_eaten_per_episode.append(env.eaten_fruits)\n",
    "\n",
    "    # if ((episode + 1) % CONFIG.save_step) == 0:\n",
    "    #     # Save episode in a gif\n",
    "    #     os.makedirs(CONFIG.output_logdir + \"/GIF\", exist_ok=True)\n",
    "    #     try:\n",
    "    #         if timestep > 1000:\n",
    "    #             output_gif = CONFIG.output_logdir + \"/GIF/game_ep_{}_1.gif\".format(\n",
    "    #                 episode\n",
    "    #             )\n",
    "    #             create_gif_from_plt_images(frames[0:500], output_gif, duration=200)\n",
    "    #             output_gif = CONFIG.output_logdir + \"/GIF/game_ep_{}_2.gif\".format(\n",
    "    #                 episode\n",
    "    #             )\n",
    "    #             create_gif_from_plt_images(frames[-500:], output_gif, duration=200)\n",
    "    #         else:\n",
    "    #             output_gif = CONFIG.output_logdir + \"/GIF/game_{}.gif\".format(\n",
    "    #                 episode\n",
    "    #             )\n",
    "    #             create_gif_from_plt_images(frames, output_gif, duration=200)\n",
    "    #     except:\n",
    "    #         pass\n",
    "\n",
    "    #     # write on file current average reward\n",
    "    #     metrics = {\n",
    "    #         \"return_queue\": env.return_queue,\n",
    "    #         \"length_queue\": env.length_queue,\n",
    "    #         \"training_error\": snake_agent.training_error,\n",
    "    #         \"epsilon\": snake_agent.epsilon,\n",
    "    #     }\n",
    "    #     file = json.load(open(filename))\n",
    "    #     file[\"episode_{}\".format(episode)] = {\n",
    "    #         \"training_error\": str(np.mean(snake_agent.training_error)),\n",
    "    #         \"mean_step\": np.mean(n_step_per_episode),\n",
    "    #         \"mean_eaten\": np.mean(fruits_eaten_per_episode),\n",
    "    #         \"mean_reward\": np.mean(rewards_per_episode),\n",
    "    #         \"eatens\": env.eaten_fruits,\n",
    "    #         \"epsilon\": str(snake_agent.epsilon),\n",
    "    #     }\n",
    "    #     json.dump(file, open(filename, \"w\"), indent=4)\n",
    "    #     fruits_eaten_per_episode = []\n",
    "    #     os.makedirs(CONFIG.output_logdir + \"/metrics\", exist_ok=True)\n",
    "    #     with open(\n",
    "    #         CONFIG.output_logdir + \"/metrics/metrics_{}\".format(episode), \"wb\"\n",
    "    #     ) as handle:\n",
    "    #         pickle.dump(metrics, handle)\n",
    "    #     # do we want to save it every 100 episodes? dunno it's up to you\n",
    "    #     snake_agent.save_model(\n",
    "    #         CONFIG.output_checkpoint_dir + \"/model_{}\".format(episode),\n",
    "    #         optimizer=optimizer,\n",
    "    #     )\n",
    "        # save_fig(env, snake_agent, episode)\n",
    "\n",
    "    # Condition to consider the task solved\n",
    "    if np.mean(env.return_queue) > 500:  # type: ignore\n",
    "        print(\"Solved at episode {}!\".format(episode))\n",
    "        break\n",
    "    pbar.update(1)\n",
    "    pbar.set_description(\"Epsilon: {}\".format(snake_agent.epsilon))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
