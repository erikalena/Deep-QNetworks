{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep QNetworks\n",
    "\n",
    "Deep Qlearning with experience replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "from qnetworks import ReplayBuffer\n",
    "from deep_qnetworks import DQN, SnakeEnv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 64  # Size of batch taken from replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the environment \n",
    "Lx = 20\n",
    "Ly = 20\n",
    "\n",
    "env = SnakeEnv(Lx,Ly)\n",
    "\n",
    "# The first model makes the predictions for Q-values which are used to make a action.\n",
    "model = DQN(in_channels =1, num_actions=env.num_actions, input_size=env.Lx)\n",
    "# The target model makes the prediction of future rewards.\n",
    "# The weights of a target model get updated every 10000 steps thus when the\n",
    "# loss between the Q-values is calculated the target Q-value is stable.\n",
    "model_target = DQN(in_channels = 1, num_actions=env.num_actions, input_size=env.Lx)\n",
    "\n",
    "model.to(device)\n",
    "model_target.to(device)\n",
    "\n",
    "# In the Deepmind paper they use RMSProp however then Adam optimizer\n",
    "# improves training time\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00025)\n",
    "# huber loss\n",
    "loss_function = nn.HuberLoss()\n",
    "\n",
    "num_actions = env.num_actions\n",
    "action_space = np.arange(num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x204b8b3aca0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# brief check to see the snake moves as expected\n",
    "\n",
    "env.reset()\n",
    "S_new, reward, done = env.single_step([0,0,2,2], 2)\n",
    "#plt.imshow(env.get_image(S_new))\n",
    "\n",
    "S_new, reward, done = env.single_step(S_new, 2)\n",
    "S_new, reward, done = env.single_step(S_new, 0)\n",
    "S_new, reward, done = env.single_step(S_new, 0)\n",
    "\n",
    "S_new, reward, done = env.single_step([S_new[0],S_new[1],5,5, S_new[4]], 0)\n",
    "plt.imshow(env.get_image(S_new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(states, actions, rewards, next_states, dones, discount):\n",
    "    \"\"\"\n",
    "    Perform a training iteration on a batch of data sampled from the experience\n",
    "    replay buffer.\n",
    "\n",
    "    Takes as input:\n",
    "        - states: a batch of states\n",
    "        - actions: a batch of actions\n",
    "        - rewards: a batch of rewards\n",
    "        - next_states: a batch of next states\n",
    "        - dones: a batch of dones\n",
    "        - discount: the discount factor, standard discount factor in RL to evaluate less long term rewards\n",
    "    \"\"\"\n",
    "\n",
    "    # compute targets for Q-learning\n",
    "    # the max Q-value of the next state is the target for the current state\n",
    "    # the image to be fed to the network is a grey scale image of the world\n",
    "    images = [env.get_image(next_state) for next_state in next_states]\n",
    "    input = torch.as_tensor(np.array(images), dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    max_next_qs = model_target(input).max(-1).values\n",
    "\n",
    "    # transform into tensors and move to device\n",
    "    rewards = torch.as_tensor(rewards, dtype=torch.float32).to(device)\n",
    "    dones = torch.as_tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "    # if the next state is terminal, then the Q-value is just the reward\n",
    "    # otherwise, we add the discounted max Q-value of the next state\n",
    "    target = rewards + (1.0 - dones) * discount * max_next_qs\n",
    "\n",
    "    # then to compute the loss, we also need the Q-value of the current state\n",
    "    images = [env.get_image(state) for state in states]\n",
    "    input = torch.as_tensor(np.array(images), dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    qs = model(input)\n",
    "\n",
    "    # for each state, we update ONLY the Q-value of the action that was taken\n",
    "    action_masks = F.one_hot(torch.as_tensor(np.array(actions)).long(), num_actions)\n",
    "    masked_qs = (action_masks * qs).sum(dim=-1)\n",
    "    loss = loss_function(masked_qs, target.detach())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the buffer, with a size of 100000, when it is full, it will remove the oldest element\n",
    "buffer = ReplayBuffer(size = 100000, device=device) \n",
    "\n",
    "cur_frame = 0\n",
    "last_100_ep_rewards = []\n",
    "last_100_ep_steps = []\n",
    "food_eaten = []\n",
    "\n",
    "max_steps_per_episode = 500\n",
    "max_num_episodes = 10000\n",
    "\n",
    "epsilon = 1\n",
    "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 4\n",
    "# How often to update the target network\n",
    "update_target_network = 10000\n",
    "\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 50000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "\n",
    "filename = 'dqn_results.txt'\n",
    "env.start = np.array([0,0])\n",
    "\n",
    "# write config to file\n",
    "with open(filename, 'w') as f:\n",
    "    f.write('Lx: ' + str(Lx) + '\\n')\n",
    "    f.write('max_steps_per_episode: ' + str(max_steps_per_episode) + '\\n')\n",
    "    f.write('max_num_episodes: ' + str(max_num_episodes) + '\\n')\n",
    "    f.write('epsilon_min: ' + str(epsilon_min) + '\\n')\n",
    "    f.write('epsilon_max: ' + str(epsilon_max) + '\\n')\n",
    "    f.write('update_after_actions: ' + str(update_after_actions) + '\\n')\n",
    "    f.write('update_target_network: ' + str(update_target_network) + '\\n')\n",
    "    f.write('epsilon_random_frames: ' + str(epsilon_random_frames) + '\\n')\n",
    "    f.write('epsilon_greedy_frames: ' + str(epsilon_greedy_frames) + '\\n')\n",
    "    f.write('episode,running_reward,epsilon,mean_steps,mean_food\\n')\n",
    "\n",
    "for episode in tqdm(range(max_num_episodes)):\n",
    "    env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    # state is a tuple of 4 values made of starting position and goal position\n",
    "    # start of an episode is always [0,0] for snake and a random position for goal\n",
    "    start_x = env.start[0]\n",
    "    start_y = env.start[1]\n",
    "    goal_x = np.random.randint(0,env.Lx)\n",
    "    goal_y = np.random.randint(0,env.Ly)\n",
    "\n",
    "    body = []\n",
    "    state = [start_x, start_y, goal_x, goal_y, []]\n",
    "\n",
    "\n",
    "    timestep = 0\n",
    "    food = 0\n",
    "    done = False\n",
    "    \n",
    "    \n",
    "    while timestep < max_steps_per_episode and not done:\n",
    "    \n",
    "        cur_frame += 1\n",
    "        action = env.select_epsilon_greedy_action(model, state, epsilon)\n",
    "        \n",
    "        next_state, reward, done = env.single_step(state, action)\n",
    "        episode_reward += reward\n",
    "    \n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        buffer.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        cur_frame += 1\n",
    "    \n",
    "        # Train neural network\n",
    "        if len(buffer) > batch_size and cur_frame % update_after_actions == 0:\n",
    "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "            loss = train_step(states, actions, rewards, next_states, dones, discount=0.99)\n",
    "        \n",
    "        # Update target network every update_target_network steps\n",
    "        if cur_frame % update_target_network == 0:\n",
    "            model_target.load_state_dict(model.state_dict())\n",
    "\n",
    "        timestep += 1\n",
    "\n",
    "        \"\"\" epsilon -= (epsilon_max - epsilon_min) / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min) \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    if len(last_100_ep_rewards) == 100:\n",
    "        last_100_ep_rewards = last_100_ep_rewards[1:]\n",
    "        last_100_ep_steps = last_100_ep_steps[1:]\n",
    "        food_eaten = food_eaten[1:]\n",
    "\n",
    "    last_100_ep_rewards.append(episode_reward)\n",
    "    last_100_ep_steps.append(timestep)\n",
    "    food_eaten.append(food)\n",
    "\n",
    "    running_reward = np.mean(last_100_ep_rewards)\n",
    "    mean_steps = np.mean(last_100_ep_steps)\n",
    "    mean_food = np.mean(food_eaten)\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        # write on file current average reward\n",
    "        with open(filename, 'a') as f:\n",
    "            f.write(f'{episode},{running_reward:.2f}, {epsilon:.3f}, {mean_steps:.3f}, {mean_food}\\n')\n",
    "\n",
    "        epsilon -= 0.025\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "    \n",
    "        \n",
    "    # Condition to consider the task solved\n",
    "    # e.g. to eat at least 6 consecutive food items\n",
    "    # without eating itself, considering also the moves to reach the food\n",
    "    if running_reward > 500: \n",
    "        print(\"Solved at episode {}!\".format(episode))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
