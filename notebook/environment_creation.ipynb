{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Make your own custom environment\n",
        "\n",
        "This documentation overviews creating new environments and relevant\n",
        "useful wrappers, utilities and tests included in Gymnasium designed for\n",
        "the creation of new environments. You can clone gym-examples to play\n",
        "with the code that is presented here. We recommend that you use a virtual environment:\n",
        "\n",
        ".. code:: console\n",
        "\n",
        "   git clone https://github.com/Farama-Foundation/gym-examples\n",
        "   cd gym-examples\n",
        "   python -m venv .env\n",
        "   source .env/bin/activate\n",
        "   pip install -e .\n",
        "\n",
        "## Subclassing gymnasium.Env\n",
        "\n",
        "Before learning how to create your own environment you should check out\n",
        "[the documentation of Gymnasium’s API](/api/env)_.\n",
        "\n",
        "We will be concerned with a subset of gym-examples that looks like this:\n",
        "\n",
        ".. code:: sh\n",
        "\n",
        "   gym-examples/\n",
        "     README.md\n",
        "     setup.py\n",
        "     gym_examples/\n",
        "       __init__.py\n",
        "       envs/\n",
        "         __init__.py\n",
        "         grid_world.py\n",
        "       wrappers/\n",
        "         __init__.py\n",
        "         relative_position.py\n",
        "         reacher_weighted_reward.py\n",
        "         discrete_action.py\n",
        "         clip_reward.py\n",
        "\n",
        "To illustrate the process of subclassing ``gymnasium.Env``, we will\n",
        "implement a very simplistic game, called ``GridWorldEnv``. We will write\n",
        "the code for our custom environment in\n",
        "``gym-examples/gym_examples/envs/grid_world.py``. The environment\n",
        "consists of a 2-dimensional square grid of fixed size (specified via the\n",
        "``size`` parameter during construction). The agent can move vertically\n",
        "or horizontally between grid cells in each timestep. The goal of the\n",
        "agent is to navigate to a target on the grid that has been placed\n",
        "randomly at the beginning of the episode.\n",
        "\n",
        "-  Observations provide the location of the target and agent.\n",
        "-  There are 4 actions in our environment, corresponding to the\n",
        "   movements “right”, “up”, “left”, and “down”.\n",
        "-  A done signal is issued as soon as the agent has navigated to the\n",
        "   grid cell where the target is located.\n",
        "-  Rewards are binary and sparse, meaning that the immediate reward is\n",
        "   always zero, unless the agent has reached the target, then it is 1.\n",
        "\n",
        "An episode in this environment (with ``size=5``) might look like this:\n",
        "\n",
        "where the blue dot is the agent and the red square represents the\n",
        "target.\n",
        "\n",
        "Let us look at the source code of ``GridWorldEnv`` piece by piece:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Declaration and Initialization\n",
        "\n",
        "Our custom environment will inherit from the abstract class\n",
        "``gymnasium.Env``. You shouldn’t forget to add the ``metadata``\n",
        "attribute to your class. There, you should specify the render-modes that\n",
        "are supported by your environment (e.g. ``\"human\"``, ``\"rgb_array\"``,\n",
        "``\"ansi\"``) and the framerate at which your environment should be\n",
        "rendered. Every environment should support ``None`` as render-mode; you\n",
        "don’t need to add it in the metadata. In ``GridWorldEnv``, we will\n",
        "support the modes “rgb_array” and “human” and render at 4 FPS.\n",
        "\n",
        "The ``__init__`` method of our environment will accept the integer\n",
        "``size``, that determines the size of the square grid. We will set up\n",
        "some variables for rendering and define ``self.observation_space`` and\n",
        "``self.action_space``. In our case, observations should provide\n",
        "information about the location of the agent and target on the\n",
        "2-dimensional grid. We will choose to represent observations in the form\n",
        "of dictionaries with keys ``\"agent\"`` and ``\"target\"``. An observation\n",
        "may look like ``{\"agent\": array([1, 0]), \"target\": array([0, 3])}``.\n",
        "Since we have 4 actions in our environment (“right”, “up”, “left”,\n",
        "“down”), we will use ``Discrete(4)`` as an action space. Here is the\n",
        "declaration of ``GridWorldEnv`` and the implementation of ``__init__``:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pygame\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "\n",
        "class GridWorldEnv(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
        "\n",
        "    def __init__(self, render_mode=None, size=5):\n",
        "        self.size = size  # The size of the square grid\n",
        "        self.window_size = 512  # The size of the PyGame window\n",
        "\n",
        "        # Observations are dictionaries with the agent's and the target's location.\n",
        "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
        "        self.observation_space = spaces.Dict(\n",
        "            {\n",
        "                \"agent\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
        "                \"target\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "\n",
        "        \"\"\"\n",
        "        The following dictionary maps abstract actions from `self.action_space` to\n",
        "        the direction we will walk in if that action is taken.\n",
        "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
        "        \"\"\"\n",
        "        self._action_to_direction = {\n",
        "            0: np.array([1, 0]),\n",
        "            1: np.array([0, 1]),\n",
        "            2: np.array([-1, 0]),\n",
        "            3: np.array([0, -1]),\n",
        "        }\n",
        "\n",
        "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        \"\"\"\n",
        "        If human-rendering is used, `self.window` will be a reference\n",
        "        to the window that we draw to. `self.clock` will be a clock that is used\n",
        "        to ensure that the environment is rendered at the correct framerate in\n",
        "        human-mode. They will remain `None` until human-mode is used for the\n",
        "        first time.\n",
        "        \"\"\"\n",
        "        self.window = None\n",
        "        self.clock = None\n",
        "        \n",
        "    def _get_obs(self):\n",
        "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
        "    \n",
        "    def _get_info(self):\n",
        "        return {\n",
        "            \"distance\": np.linalg.norm(\n",
        "                self._agent_location - self._target_location, ord=1\n",
        "            )\n",
        "        }\n",
        "    \n",
        "    def reset(self, seed=None, options=None):\n",
        "        # We need the following line to seed self.np_random\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        # Choose the agent's location uniformly at random\n",
        "        self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
        "\n",
        "        # We will sample the target's location randomly until it does not coincide with the agent's location\n",
        "        self._target_location = self._agent_location\n",
        "        while np.array_equal(self._target_location, self._agent_location):\n",
        "            self._target_location = self.np_random.integers(\n",
        "                0, self.size, size=2, dtype=int\n",
        "            )\n",
        "\n",
        "        observation = self._get_obs()\n",
        "        info = self._get_info()\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self._render_frame()\n",
        "\n",
        "        return observation, info\n",
        "    def step(self, action):\n",
        "        # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
        "        direction = self._action_to_direction[action]\n",
        "        # We use `np.clip` to make sure we don't leave the grid\n",
        "        self._agent_location = np.clip(\n",
        "            self._agent_location + direction, 0, self.size - 1\n",
        "        )\n",
        "        # An episode is done iff the agent has reached the target\n",
        "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
        "        reward = 1 if terminated else 0  # Binary sparse rewards\n",
        "        observation = self._get_obs()\n",
        "        info = self._get_info()\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self._render_frame()\n",
        "\n",
        "        return observation, reward, terminated, False, info\n",
        "    \n",
        "    def render(self):\n",
        "        if self.render_mode == \"rgb_array\":\n",
        "            return self._render_frame()\n",
        "\n",
        "    def _render_frame(self):\n",
        "        if self.window is None and self.render_mode == \"human\":\n",
        "            pygame.init()\n",
        "            pygame.display.init()\n",
        "            self.window = pygame.display.set_mode(\n",
        "                (self.window_size, self.window_size)\n",
        "            )\n",
        "        if self.clock is None and self.render_mode == \"human\":\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
        "        canvas.fill((255, 255, 255))\n",
        "        pix_square_size = (\n",
        "            self.window_size / self.size\n",
        "        )  # The size of a single grid square in pixels\n",
        "\n",
        "        # First we draw the target\n",
        "        pygame.draw.rect(\n",
        "            canvas,\n",
        "            (255, 0, 0),\n",
        "            pygame.Rect(\n",
        "                pix_square_size * self._target_location,\n",
        "                (pix_square_size, pix_square_size),\n",
        "            ),\n",
        "        )\n",
        "        # Now we draw the agent\n",
        "        pygame.draw.circle(\n",
        "            canvas,\n",
        "            (0, 0, 255),\n",
        "            (self._agent_location + 0.5) * pix_square_size,\n",
        "            pix_square_size / 3,\n",
        "        )\n",
        "\n",
        "        # Finally, add some gridlines\n",
        "        for x in range(self.size + 1):\n",
        "            pygame.draw.line(\n",
        "                canvas,\n",
        "                0,\n",
        "                (0, pix_square_size * x),\n",
        "                (self.window_size, pix_square_size * x),\n",
        "                width=3,\n",
        "            )\n",
        "            pygame.draw.line(\n",
        "                canvas,\n",
        "                0,\n",
        "                (pix_square_size * x, 0),\n",
        "                (pix_square_size * x, self.window_size),\n",
        "                width=3,\n",
        "            )\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            # The following line copies our drawings from `canvas` to the visible window\n",
        "            self.window.blit(canvas, canvas.get_rect())\n",
        "            pygame.event.pump()\n",
        "            pygame.display.update()\n",
        "\n",
        "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
        "            # The following line will automatically add a delay to keep the framerate stable.\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "        else:  # rgb_array\n",
        "            return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
        "            )\n",
        "    def close(self):\n",
        "        if self.window is not None:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = GridWorldEnv(size=10, render_mode=\"human\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "envs =  gym.vector.AsyncVectorEnv([\n",
        "        lambda: GridWorldEnv(size=10, render_mode=\"human\")\n",
        "        for _ in range(3)\n",
        "    ], context='forkserver')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(OrderedDict([('agent',\n",
              "               array([[3, 8],\n",
              "                      [2, 7],\n",
              "                      [0, 9]])),\n",
              "              ('target',\n",
              "               array([[5, 3],\n",
              "                      [8, 9],\n",
              "                      [1, 3]]))]),\n",
              " {'distance': array([7., 8., 7.]), '_distance': array([ True,  True,  True])})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "envs.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Constructing Observations From Environment States\n",
        "\n",
        "Since we will need to compute observations both in ``reset`` and\n",
        "``step``, it is often convenient to have a (private) method ``_get_obs``\n",
        "that translates the environment’s state into an observation. However,\n",
        "this is not mandatory and you may as well compute observations in\n",
        "``reset`` and ``step`` separately:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also implement a similar method for the auxiliary information\n",
        "that is returned by ``step`` and ``reset``. In our case, we would like\n",
        "to provide the manhattan distance between the agent and the target:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Oftentimes, info will also contain some data that is only available\n",
        "inside the ``step`` method (e.g. individual reward terms). In that case,\n",
        "we would have to update the dictionary that is returned by ``_get_info``\n",
        "in ``step``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reset\n",
        "\n",
        "The ``reset`` method will be called to initiate a new episode. You may\n",
        "assume that the ``step`` method will not be called before ``reset`` has\n",
        "been called. Moreover, ``reset`` should be called whenever a done signal\n",
        "has been issued. Users may pass the ``seed`` keyword to ``reset`` to\n",
        "initialize any random number generator that is used by the environment\n",
        "to a deterministic state. It is recommended to use the random number\n",
        "generator ``self.np_random`` that is provided by the environment’s base\n",
        "class, ``gymnasium.Env``. If you only use this RNG, you do not need to\n",
        "worry much about seeding, *but you need to remember to call\n",
        "``super().reset(seed=seed)``* to make sure that ``gymnasium.Env``\n",
        "correctly seeds the RNG. Once this is done, we can randomly set the\n",
        "state of our environment. In our case, we randomly choose the agent’s\n",
        "location and the random sample target positions, until it does not\n",
        "coincide with the agent’s position.\n",
        "\n",
        "The ``reset`` method should return a tuple of the initial observation\n",
        "and some auxiliary information. We can use the methods ``_get_obs`` and\n",
        "``_get_info`` that we implemented earlier for that:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step\n",
        "\n",
        "The ``step`` method usually contains most of the logic of your\n",
        "environment. It accepts an ``action``, computes the state of the\n",
        "environment after applying that action and returns the 4-tuple\n",
        "``(observation, reward, done, info)``. Once the new state of the\n",
        "environment has been computed, we can check whether it is a terminal\n",
        "state and we set ``done`` accordingly. Since we are using sparse binary\n",
        "rewards in ``GridWorldEnv``, computing ``reward`` is trivial once we\n",
        "know ``done``. To gather ``observation`` and ``info``, we can again make\n",
        "use of ``_get_obs`` and ``_get_info``:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rendering\n",
        "\n",
        "Here, we are using PyGame for rendering. A similar approach to rendering\n",
        "is used in many environments that are included with Gymnasium and you\n",
        "can use it as a skeleton for your own environments:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def render(self):\n",
        "        if self.render_mode == \"rgb_array\":\n",
        "            return self._render_frame()\n",
        "\n",
        "    def _render_frame(self):\n",
        "        if self.window is None and self.render_mode == \"human\":\n",
        "            pygame.init()\n",
        "            pygame.display.init()\n",
        "            self.window = pygame.display.set_mode(\n",
        "                (self.window_size, self.window_size)\n",
        "            )\n",
        "        if self.clock is None and self.render_mode == \"human\":\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
        "        canvas.fill((255, 255, 255))\n",
        "        pix_square_size = (\n",
        "            self.window_size / self.size\n",
        "        )  # The size of a single grid square in pixels\n",
        "\n",
        "        # First we draw the target\n",
        "        pygame.draw.rect(\n",
        "            canvas,\n",
        "            (255, 0, 0),\n",
        "            pygame.Rect(\n",
        "                pix_square_size * self._target_location,\n",
        "                (pix_square_size, pix_square_size),\n",
        "            ),\n",
        "        )\n",
        "        # Now we draw the agent\n",
        "        pygame.draw.circle(\n",
        "            canvas,\n",
        "            (0, 0, 255),\n",
        "            (self._agent_location + 0.5) * pix_square_size,\n",
        "            pix_square_size / 3,\n",
        "        )\n",
        "\n",
        "        # Finally, add some gridlines\n",
        "        for x in range(self.size + 1):\n",
        "            pygame.draw.line(\n",
        "                canvas,\n",
        "                0,\n",
        "                (0, pix_square_size * x),\n",
        "                (self.window_size, pix_square_size * x),\n",
        "                width=3,\n",
        "            )\n",
        "            pygame.draw.line(\n",
        "                canvas,\n",
        "                0,\n",
        "                (pix_square_size * x, 0),\n",
        "                (pix_square_size * x, self.window_size),\n",
        "                width=3,\n",
        "            )\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            # The following line copies our drawings from `canvas` to the visible window\n",
        "            self.window.blit(canvas, canvas.get_rect())\n",
        "            pygame.event.pump()\n",
        "            pygame.display.update()\n",
        "\n",
        "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
        "            # The following line will automatically add a delay to keep the framerate stable.\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "        else:  # rgb_array\n",
        "            return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Close\n",
        "\n",
        "The ``close`` method should close any open resources that were used by\n",
        "the environment. In many cases, you don’t actually have to bother to\n",
        "implement this method. However, in our example ``render_mode`` may be\n",
        "``\"human\"`` and we might need to close the window that has been opened:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def close(self):\n",
        "        if self.window is not None:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In other environments ``close`` might also close files that were opened\n",
        "or release other resources. You shouldn’t interact with the environment\n",
        "after having called ``close``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Registering Envs\n",
        "\n",
        "In order for the custom environments to be detected by Gymnasium, they\n",
        "must be registered as follows. We will choose to put this code in\n",
        "``gym-examples/gym_examples/__init__.py``.\n",
        "\n",
        ".. code:: python\n",
        "\n",
        "  from gymnasium.envs.registration import register\n",
        "\n",
        "  register(\n",
        "       id=\"gym_examples/GridWorld-v0\",\n",
        "       entry_point=\"gym_examples.envs:GridWorldEnv\",\n",
        "       max_episode_steps=300,\n",
        "  )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The environment ID consists of three components, two of which are\n",
        "optional: an optional namespace (here: ``gym_examples``), a mandatory\n",
        "name (here: ``GridWorld``) and an optional but recommended version\n",
        "(here: v0). It might have also been registered as ``GridWorld-v0`` (the\n",
        "recommended approach), ``GridWorld`` or ``gym_examples/GridWorld``, and\n",
        "the appropriate ID should then be used during environment creation.\n",
        "\n",
        "The keyword argument ``max_episode_steps=300`` will ensure that\n",
        "GridWorld environments that are instantiated via ``gymnasium.make`` will\n",
        "be wrapped in a ``TimeLimit`` wrapper (see [the wrapper\n",
        "documentation](/api/wrappers)_ for more information). A done signal\n",
        "will then be produced if the agent has reached the target *or* 300 steps\n",
        "have been executed in the current episode. To distinguish truncation and\n",
        "termination, you can check ``info[\"TimeLimit.truncated\"]``.\n",
        "\n",
        "Apart from ``id`` and ``entrypoint``, you may pass the following\n",
        "additional keyword arguments to ``register``:\n",
        "\n",
        "+----------------------+-----------+-----------+---------------------------------------------------------------------------------------------------------------+\n",
        "| Name                 | Type      | Default   | Description                                                                                                   |\n",
        "+======================+===========+===========+===============================================================================================================+\n",
        "| ``reward_threshold`` | ``float`` | ``None``  | The reward threshold before the task is  considered solved                                                    |\n",
        "+----------------------+-----------+-----------+---------------------------------------------------------------------------------------------------------------+\n",
        "| ``nondeterministic`` | ``bool``  | ``False`` | Whether this environment is non-deterministic even after seeding                                              |\n",
        "+----------------------+-----------+-----------+---------------------------------------------------------------------------------------------------------------+\n",
        "| ``max_episode_steps``| ``int``   | ``None``  | The maximum number of steps that an episode can consist of. If not ``None``, a ``TimeLimit`` wrapper is added |\n",
        "+----------------------+-----------+-----------+---------------------------------------------------------------------------------------------------------------+\n",
        "| ``order_enforce``    | ``bool``  | ``True``  | Whether to wrap the environment in an  ``OrderEnforcing`` wrapper                                             |\n",
        "+----------------------+-----------+-----------+---------------------------------------------------------------------------------------------------------------+\n",
        "| ``autoreset``        | ``bool``  | ``False`` | Whether to wrap the environment in an ``AutoResetWrapper``                                                    |\n",
        "+----------------------+-----------+-----------+---------------------------------------------------------------------------------------------------------------+\n",
        "| ``kwargs``           | ``dict``  | ``{}``    | The default kwargs to pass to the environment class                                                           |\n",
        "+----------------------+-----------+-----------+---------------------------------------------------------------------------------------------------------------+\n",
        "\n",
        "Most of these keywords (except for ``max_episode_steps``,\n",
        "``order_enforce`` and ``kwargs``) do not alter the behavior of\n",
        "environment instances but merely provide some extra information about\n",
        "your environment. After registration, our custom ``GridWorldEnv``\n",
        "environment can be created with\n",
        "``env = gymnasium.make('gym_examples/GridWorld-v0')``.\n",
        "\n",
        "``gym-examples/gym_examples/envs/__init__.py`` should have:\n",
        "\n",
        ".. code:: python\n",
        "\n",
        "   from gym_examples.envs.grid_world import GridWorldEnv\n",
        "\n",
        "If your environment is not registered, you may optionally pass a module\n",
        "to import, that would register your environment before creating it like\n",
        "this - ``env = gymnasium.make('module:Env-v0')``, where ``module``\n",
        "contains the registration code. For the GridWorld env, the registration\n",
        "code is run by importing ``gym_examples`` so if it were not possible to\n",
        "import gym_examples explicitly, you could register while making by\n",
        "``env = gymnasium.make('gym_examples:gym_examples/GridWorld-v0)``. This\n",
        "is especially useful when you’re allowed to pass only the environment ID\n",
        "into a third-party codebase (eg. learning library). This lets you\n",
        "register your environment without needing to edit the library’s source\n",
        "code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a Package\n",
        "\n",
        "The last step is to structure our code as a Python package. This\n",
        "involves configuring ``gym-examples/setup.py``. A minimal example of how\n",
        "to do so is as follows:\n",
        "\n",
        ".. code:: python\n",
        "\n",
        "   from setuptools import setup\n",
        "\n",
        "   setup(\n",
        "       name=\"gym_examples\",\n",
        "       version=\"0.0.1\",\n",
        "       install_requires=[\"gymnasium==0.26.0\", \"pygame==2.1.0\"],\n",
        "   )\n",
        "\n",
        "## Creating Environment Instances\n",
        "\n",
        "After you have installed your package locally with\n",
        "``pip install -e gym-examples``, you can create an instance of the\n",
        "environment via:\n",
        "\n",
        ".. code:: python\n",
        "\n",
        "   import gym_examples\n",
        "   env = gymnasium.make('gym_examples/GridWorld-v0')\n",
        "\n",
        "You can also pass keyword arguments of your environment’s constructor to\n",
        "``gymnasium.make`` to customize the environment. In our case, we could\n",
        "do:\n",
        "\n",
        ".. code:: python\n",
        "\n",
        "   env = gymnasium.make('gym_examples/GridWorld-v0', size=10)\n",
        "\n",
        "Sometimes, you may find it more convenient to skip registration and call\n",
        "the environment’s constructor yourself. Some may find this approach more\n",
        "pythonic and environments that are instantiated like this are also\n",
        "perfectly fine (but remember to add wrappers as well!).\n",
        "\n",
        "## Using Wrappers\n",
        "\n",
        "Oftentimes, we want to use different variants of a custom environment,\n",
        "or we want to modify the behavior of an environment that is provided by\n",
        "Gymnasium or some other party. Wrappers allow us to do this without\n",
        "changing the environment implementation or adding any boilerplate code.\n",
        "Check out the [wrapper documentation](/api/wrappers/)_ for details on\n",
        "how to use wrappers and instructions for implementing your own. In our\n",
        "example, observations cannot be used directly in learning code because\n",
        "they are dictionaries. However, we don’t actually need to touch our\n",
        "environment implementation to fix this! We can simply add a wrapper on\n",
        "top of environment instances to flatten observations into a single\n",
        "array:\n",
        "\n",
        ".. code:: python\n",
        "\n",
        "   import gym_examples\n",
        "   from gymnasium.wrappers import FlattenObservation\n",
        "\n",
        "   env = gymnasium.make('gym_examples/GridWorld-v0')\n",
        "   wrapped_env = FlattenObservation(env)\n",
        "   print(wrapped_env.reset())     # E.g.  [3 0 3 3], {}\n",
        "\n",
        "Wrappers have the big advantage that they make environments highly\n",
        "modular. For instance, instead of flattening the observations from\n",
        "GridWorld, you might only want to look at the relative position of the\n",
        "target and the agent. In the section on\n",
        "[ObservationWrappers](/api/wrappers/#observationwrapper)_ we have\n",
        "implemented a wrapper that does this job. This wrapper is also available\n",
        "in gym-examples:\n",
        "\n",
        ".. code:: python\n",
        "\n",
        "   import gym_examples\n",
        "   from gym_examples.wrappers import RelativePosition\n",
        "\n",
        "   env = gymnasium.make('gym_examples/GridWorld-v0')\n",
        "   wrapped_env = RelativePosition(env)\n",
        "   print(wrapped_env.reset())     # E.g.  [-3  3], {}\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
